# scripts/build_market_vectordb.py
"""
PDF â†’ FAISS ë³€í™˜ (LangChain ì—†ì´)
ì‹œì¥ ë¦¬ì„œì¹˜ ë³´ê³ ì„œë¥¼ FAISS ë²¡í„° DBë¡œ ë³€í™˜
"""

import os
import pickle
from pathlib import Path
from typing import List, Dict, Any
import PyPDF2
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss


# ì‚°ì—…ë³„ í‚¤ì›Œë“œ
INDUSTRY_KEYWORDS = {
    "Healthcare": ["health", "medical", "patient", "diagnosis", "clinical", "hospital", "healthcare", "ì˜ë£Œ", "í™˜ì"],
    "Finance": ["finance", "banking", "payment", "fintech", "investment", "trading", "ê¸ˆìœµ", "ì€í–‰"],
    "Marketing": ["marketing", "advertising", "customer", "brand", "campaign", "ë§ˆì¼€íŒ…", "ê´‘ê³ "],
    "Education": ["education", "learning", "student", "teacher", "course", "êµìœ¡", "í•™ìŠµ"],
    "Gaming": ["game", "gaming", "player", "esports", "ê²Œì„"],
    "Media": ["media", "content", "video", "audio", "entertainment", "ë¯¸ë””ì–´", "ì½˜í…ì¸ "],
}


class Document:
    """ë¬¸ì„œ í´ë˜ìŠ¤"""
    def __init__(self, page_content: str, metadata: Dict[str, Any]):
        self.page_content = page_content
        self.metadata = metadata


def extract_text_from_pdf(pdf_path: str) -> List[Document]:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í˜ì´ì§€ë³„)"""
    documents = []
    
    print(f"ğŸ“„ PDF ì½ëŠ” ì¤‘: {pdf_path}")
    
    with open(pdf_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        total_pages = len(pdf_reader.pages)
        
        print(f"  ì´ {total_pages}í˜ì´ì§€")
        
        for page_num, page in enumerate(pdf_reader.pages, start=1):
            text = page.extract_text()
            
            if text.strip():  # ë¹ˆ í˜ì´ì§€ ì œì™¸
                # ì‚°ì—… íƒœê¹…
                industries = tag_industries(text)
                
                doc = Document(
                    page_content=text,
                    metadata={
                        "source_file": os.path.basename(pdf_path),
                        "page": page_num,
                        "industries": industries if industries else ["General"]
                    }
                )
                documents.append(doc)
                
                if page_num % 10 == 0:
                    print(f"  âœ“ {page_num}/{total_pages} í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ")
    
    print(f"âœ… ì´ {len(documents)}ê°œ í˜ì´ì§€ ì¶”ì¶œ ì™„ë£Œ")
    return documents


def tag_industries(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ì‚°ì—… íƒœê¹…"""
    text_lower = text.lower()
    matched_industries = []
    
    for industry, keywords in INDUSTRY_KEYWORDS.items():
        if any(keyword.lower() in text_lower for keyword in keywords):
            matched_industries.append(industry)
    
    return matched_industries


def split_documents(documents: List[Document], chunk_size: int = 800, overlap: int = 150) -> List[Document]:
    """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• """
    chunks = []
    
    print(f"âœ‚ï¸ ë¬¸ì„œ ë¶„í•  ì¤‘ (ì²­í¬ í¬ê¸°: {chunk_size}, ì˜¤ë²„ë©: {overlap})")
    
    for doc in documents:
        text = doc.page_content
        
        # ì²­í¬ ìƒì„±
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk_text = text[start:end]
            
            # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ê¹Œì§€ë§Œ
            if end < len(text):
                last_period = chunk_text.rfind('.')
                if last_period > chunk_size // 2:  # ì²­í¬ì˜ ì ˆë°˜ ì´ìƒì—ì„œ ì°¾ìœ¼ë©´
                    end = start + last_period + 1
                    chunk_text = text[start:end]
            
            chunks.append(Document(
                page_content=chunk_text.strip(),
                metadata=doc.metadata.copy()
            ))
            
            start = end - overlap
    
    print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
    return chunks


def create_faiss_index(
    documents: List[Document],
    model_name: str = "BAAI/bge-base-en-v1.5",
    output_dir: str = "./faiss_market_index"
) -> None:
    """FAISS ì¸ë±ìŠ¤ ìƒì„±"""
    
    print(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
    model = SentenceTransformer(model_name)
    
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    texts = [doc.page_content for doc in documents]
    
    print(f"ğŸ”„ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘...")
    embeddings = model.encode(
        texts,
        normalize_embeddings=True,
        show_progress_bar=True,
        batch_size=32
    )
    
    # FAISS ì¸ë±ìŠ¤ ìƒì„±
    dimension = embeddings.shape[1]
    print(f"ğŸ”„ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘ (ì°¨ì›: {dimension})")
    
    index = faiss.IndexFlatIP(dimension)  # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    index.add(embeddings.astype('float32'))
    
    # ì €ì¥
    os.makedirs(output_dir, exist_ok=True)
    
    # FAISS ì¸ë±ìŠ¤ ì €ì¥
    index_path = os.path.join(output_dir, "index.faiss")
    faiss.write_index(index, index_path)
    print(f"ğŸ’¾ FAISS ì¸ë±ìŠ¤ ì €ì¥: {index_path}")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        "documents": [
            {
                "page_content": doc.page_content,
                "metadata": doc.metadata
            }
            for doc in documents
        ],
        "model_name": model_name
    }
    
    metadata_path = os.path.join(output_dir, "index.pkl")
    with open(metadata_path, 'wb') as f:
        pickle.dump(metadata, f)
    print(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
    
    print(f"âœ… FAISS DB ìƒì„± ì™„ë£Œ: {output_dir}")
    print(f"  - ì´ ë¬¸ì„œ: {len(documents)}ê°œ")
    print(f"  - ë²¡í„° ì°¨ì›: {dimension}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # PDF ê²½ë¡œ (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
    pdf_path = "./data/ai-dossier-r.pdf"
    
    if not os.path.exists(pdf_path):
        print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        print(f"   './data/ai-dossier-r.pdf' ê²½ë¡œì— PDFë¥¼ ë°°ì¹˜í•˜ì„¸ìš”.")
        return
    
    print("=" * 60)
    print("ğŸš€ ì‹œì¥ ë¦¬ì„œì¹˜ FAISS DB ìƒì„± ì‹œì‘")
    print("=" * 60)
    
    # 1. PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    documents = extract_text_from_pdf(pdf_path)
    
    # 2. ì²­í¬ë¡œ ë¶„í• 
    chunks = split_documents(documents, chunk_size=800, overlap=150)
    
    # 3. FAISS ì¸ë±ìŠ¤ ìƒì„±
    create_faiss_index(
        chunks,
        model_name="BAAI/bge-base-en-v1.5",
        output_dir="./faiss_market_index"
    )
    
    print("\n" + "=" * 60)
    print("âœ… ì™„ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    main()