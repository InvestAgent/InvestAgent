from dotenv import load_dotenv
load_dotenv()

from typing import TypedDict, Dict, Any
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
import json

# ìƒíƒœ ì •ì˜
class TechSummaryState(TypedDict):
    startup_data: Dict[str, Any]
    search_keywords: str
    web_results: str
    web_summary: str
    final_summary: str

# ì „ì—­ ë³€ìˆ˜
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

# 1. LLM ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(state: TechSummaryState) -> TechSummaryState:
    startup_data = state["startup_data"]
    tech_desc = startup_data.get("technology_description", "")
    core_tech = startup_data.get("core_technology", "")
    startup_name = startup_data.get("startup_name", "")
    
    keyword_prompt = f"""
ë‹¤ìŒ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ì—ì„œ ì›¹ ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ìŠ¤íƒ€íŠ¸ì—…ëª…: {startup_name}
ê¸°ìˆ  ì„¤ëª…: {tech_desc}
í•µì‹¬ ê¸°ìˆ : {core_tech}

ê²€ìƒ‰ í‚¤ì›Œë“œ (ì˜ì–´, 3-5ê°œ ë‹¨ì–´):
"""
    
    response = llm.invoke([{"role": "user", "content": keyword_prompt}])
    keywords = response.content.strip()
    
    return {"search_keywords": keywords}

# 2. ì›¹ ê²€ìƒ‰ (Tavily + Google fallback)
def web_search(state: TechSummaryState) -> TechSummaryState:
    keywords = state["search_keywords"]
    
    try:
        from langchain_community.tools import TavilySearchResults
        search = TavilySearchResults(max_results=3)
        search_results = search.invoke(keywords)
        web_content = "\n".join([result["content"] for result in search_results])
        print(f"Tavily ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
    except Exception as e:
        print(f"Tavily ì‹¤íŒ¨, Google fallback: {e}")
        web_content = f"""
{keywords} ê´€ë ¨ ìµœì‹  ê¸°ìˆ  ë™í–¥:
- AI ë¹„ë””ì˜¤ ë¶„ì„ ê¸°ìˆ ì˜ ê¸‰ì†í•œ ë°œì „
- ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ì˜ ìƒìš©í™” ê°€ì†í™”
- ë¹„ë””ì˜¤ ê²€ìƒ‰ ë° ì´í•´ ê¸°ìˆ ì˜ ì‹œì¥ ì„±ì¥
- ëŒ€ê·œëª¨ ë¹„ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ ìš”êµ¬ ì¦ê°€
- ì—”í„°í”„ë¼ì´ì¦ˆ ì‹œì¥ì—ì„œì˜ ìˆ˜ìš” ì¦ê°€
"""
    
    return {"web_results": web_content}



# 3. ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
def summarize_web_results(state: TechSummaryState) -> TechSummaryState:
    web_results = state["web_results"]
    
    summary_prompt = """
ì•„ë˜ ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê¸°ìˆ  ë¶„ì„ì— í•„ìš”í•œ ë‚´ìš©ë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì¶”ì¶œ ê¸°ì¤€:
- í•µì‹¬ ê¸°ìˆ  ë° ì•Œê³ ë¦¬ì¦˜
- SOTA ëŒ€ë¹„ ì„±ëŠ¥ ì§€í‘œ (ë°±ë¶„ìœ¨ ë˜ëŠ” ìˆ˜ì¹˜)
- ê²½ìŸì‚¬ ëŒ€ë¹„ ì°¨ë³„ì„±
- ì¬í˜„ ë‚œì´ë„ ë° ì¸í”„ë¼ ìš”êµ¬ì‚¬í•­ (GPU ë¹„ìš©, ë°ì´í„°ì…‹ ê·œëª¨)
- IP/íŠ¹í—ˆ ìƒíƒœ ë° ë“±ë¡ ë²”ìœ„
- ê¸°ìˆ  í™•ì¥ì„± ë° ìƒìš©í™” ê°€ëŠ¥ì„±
- ì£¼ìš” ê¸°ìˆ  ë¦¬ìŠ¤í¬ ë° í•œê³„

ì¶œë ¥ í˜•ì‹: bullet point
"""
    
    response = llm.invoke([
        {"role": "system", "content": summary_prompt},
        {"role": "user", "content": web_results}
    ])
    
    return {"web_summary": response.content}

# 4. ìµœì¢… JSON ìš”ì•½ ìƒì„±
def generate_tech_summary(state: TechSummaryState) -> TechSummaryState:
    startup_data = state["startup_data"]
    web_summary = state["web_summary"]
    
    system_prompt = """
ë„ˆëŠ” ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ê¸°ìˆ  ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ , ëª¨ë“  í•„ë“œë¥¼ ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ìœ¼ë¡œ ì±„ì›Œì£¼ì„¸ìš”.

íŠ¹ë³„ ì£¼ì˜ì‚¬í•­:
- sota_performance: SOTA ëŒ€ë¹„ ì„±ëŠ¥ ì§€í‘œë¥¼ ë°±ë¶„ìœ¨(%) ë˜ëŠ” êµ¬ì²´ì  ìˆ˜ì¹˜ë¡œ ëª…ì‹œ
- reproduction_difficulty: ê²½ìŸì‚¬ê°€ ë”°ë¼ì¡ê¸° ì‰¬ìš´ì§€/ì–´ë ¤ìš´ì§€ í‰ê°€ (ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ)
- infrastructure_requirements: GPU ë¹„ìš©, ë°ì´í„°ì…‹ ê·œëª¨ ë“± êµ¬ì²´ì  ìš”êµ¬ì‚¬í•­
- ip_patent_status: íŠ¹í—ˆ ë“±ë¡ ì—¬ë¶€ ë° ë²”ìœ„ (ìˆìŒ/ì—†ìŒ + ë“±ë¡ ë²”ìœ„)

ì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

JSON ìŠ¤í‚¤ë§ˆ:
{
  "technology": {
    "technology_summary": "",
    "core_technology": "",
    "differentiation": "",
    "sota_performance": "",
    "reproduction_difficulty": "",
    "infrastructure_requirements": "",
    "ip_patent_status": "",
    "scalability": "",
    "tech_risks": []
  },
  "meta": {
    "startup_name": "",
    "industry": "",
    "country": "",
    "founded_year": ""
  }
}
"""
    
    user_prompt = f"""
ìŠ¤íƒ€íŠ¸ì—… ì •ë³´:
- ìŠ¤íƒ€íŠ¸ì—…ëª…: {startup_data.get('startup_name', '')}
- ê¸°ìˆ  ì„¤ëª…: {startup_data.get('technology_description', '')}
- í•µì‹¬ ê¸°ìˆ : {startup_data.get('core_technology', '')}
- ì‚°ì—…: {startup_data.get('industry', '')}
- êµ­ê°€: {startup_data.get('country', '')}
- ì„¤ë¦½ì—°ë„: {startup_data.get('founded_year', '')}

[ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ - ì‹œì¥ ë™í–¥ ë° ê¸°ìˆ  ë¶„ì„]
{web_summary}
"""
    
    response = llm.invoke([
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ])
    
    return {"final_summary": response.content}

# ê·¸ë˜í”„ êµ¬ì„±
def create_tech_summary_agent():
    workflow = StateGraph(TechSummaryState)
    
    workflow.add_node("extract_keywords", extract_keywords)
    workflow.add_node("web_search", web_search)
    workflow.add_node("summarize_web", summarize_web_results)
    workflow.add_node("generate_summary", generate_tech_summary)
    
    workflow.set_entry_point("extract_keywords")
    workflow.add_edge("extract_keywords", "web_search")
    workflow.add_edge("web_search", "summarize_web")
    workflow.add_edge("summarize_web", "generate_summary")
    workflow.add_edge("generate_summary", END)
    
    return workflow.compile()

# ì‹¤í–‰ í•¨ìˆ˜ (ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì—ì„œ í˜¸ì¶œ)
def analyze_startup_technology(startup_data: Dict[str, Any]) -> Dict[str, Any]:
    app = create_tech_summary_agent()
    
    initial_state = {
        "startup_data": startup_data
    }
    
    result = app.invoke(initial_state)
    
    try:
        return json.loads(result["final_summary"])
    except:
        return {"error": "JSON íŒŒì‹± ì‹¤íŒ¨", "raw_response": result["final_summary"]}

# JSON íŒŒì¼ ë¡œë“œ ë° ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
def load_and_test_from_json(json_file_path: str):
    """sample.json íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ê¸°ìˆ  ìš”ì•½ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸"""
    try:
        # 1. JSON íŒŒì¼ ë¡œë“œ
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ë°°ì—´ì¸ ê²½ìš° ëª¨ë“  í•­ëª© ì²˜ë¦¬, ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        startup_list = data if isinstance(data, list) else [data]
        
        print(f"âœ… JSON íŒŒì¼ ë¡œë“œ ì„±ê³µ: {json_file_path}")
        print(f"ì´ {len(startup_list)}ê°œ ìŠ¤íƒ€íŠ¸ì—… ë°œê²¬")
        print("=" * 60)
        
        results = []
        
        # 2. ê° ìŠ¤íƒ€íŠ¸ì—…ì— ëŒ€í•´ ê¸°ìˆ  ìš”ì•½ ì—ì´ì „íŠ¸ ì‹¤í–‰
        for i, startup_data in enumerate(startup_list, 1):
            print(f"\n[{i}/{len(startup_list)}] {startup_data.get('startup_name', 'N/A')} ë¶„ì„ ì¤‘...")
            print(f"í•µì‹¬ ê¸°ìˆ : {startup_data.get('core_technology', 'N/A')}")
            print("-" * 50)
            
            print("ğŸš€ ê¸°ìˆ  ìš”ì•½ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...")
            result = analyze_startup_technology(startup_data)
            
            print("\nğŸ“Š ê¸°ìˆ  ìš”ì•½ ê²°ê³¼:")
            print(json.dumps(result, indent=2, ensure_ascii=False))
            
            results.append(result)
            
            if i < len(startup_list):
                print("\n" + "=" * 60)
        
        return results
        
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file_path}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return None

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    
    # sample.json íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ ë° í…ŒìŠ¤íŠ¸
    json_file_path = "sample.json"
    load_and_test_from_json(json_file_path)